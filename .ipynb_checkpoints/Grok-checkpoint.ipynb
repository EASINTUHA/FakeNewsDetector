{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "078c0792-ca3d-436f-a2d7-6fbbf6981842",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('fake_or_real_news.csv')  # Replace with your dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afcb6ba2-7d52-468f-89f9-6c14178d21f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4605531b-e29f-4e44-870f-dde853debe8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreprocess_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mcleaned_title\u001b[39m\u001b[33m'\u001b[39m] = data[\u001b[33m'\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m'\u001b[39m].apply(preprocess_text)\n\u001b[32m      3\u001b[39m data[\u001b[33m'\u001b[39m\u001b[33mcombined_text\u001b[39m\u001b[33m'\u001b[39m] = data[\u001b[33m'\u001b[39m\u001b[33mcleaned_title\u001b[39m\u001b[33m'\u001b[39m] + \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m + data[\u001b[33m'\u001b[39m\u001b[33mcleaned_text\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4924\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4789\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4790\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4791\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4796\u001b[39m     **kwargs,\n\u001b[32m   4797\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4798\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4799\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4800\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4915\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4916\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4917\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4920\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4922\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4924\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1427\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1427\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1507\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1501\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1503\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1504\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1505\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1506\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1507\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1509\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1512\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1513\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mpreprocess_text\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      9\u001b[39m text = re.sub(\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m[^a-zA-Z\u001b[39m\u001b[33m\\\u001b[39m\u001b[33ms]\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m, text)\n\u001b[32m     10\u001b[39m words = text.split()\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m words = [\u001b[43mlemmatizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m.join(words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\stem\\wordnet.py:85\u001b[39m, in \u001b[36mWordNetLemmatizer.lemmatize\u001b[39m\u001b[34m(self, word, pos)\u001b[39m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m = \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m     61\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Lemmatize `word` by picking the shortest of the possible lemmas,\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m    using the wordnet corpus reader's built-in _morphy function.\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     83\u001b[39m \u001b[33;03m    :return: The shortest lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     lemmas = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key=\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\stem\\wordnet.py:41\u001b[39m, in \u001b[36mWordNetLemmatizer._morphy\u001b[39m\u001b[34m(self, form, pos, check_exceptions)\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[33;03m_morphy() is WordNet's _morphy lemmatizer.\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[33;03mIt returns a list of all lemmas found in WordNet.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m \u001b[33;03m['us', 'u']\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wordnet \u001b[38;5;28;01mas\u001b[39;00m wn\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_morphy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_exceptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2109\u001b[39m, in \u001b[36mWordNetCorpusReader._morphy\u001b[39m\u001b[34m(self, form, pos, check_exceptions)\u001b[39m\n\u001b[32m   2106\u001b[39m     forms = apply_rules([form])\n\u001b[32m   2108\u001b[39m \u001b[38;5;66;03m# 2. Return all that are in the database (and check the original too)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2109\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfilter_forms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mform\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mforms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:2098\u001b[39m, in \u001b[36mWordNetCorpusReader._morphy.<locals>.filter_forms\u001b[39m\u001b[34m(forms)\u001b[39m\n\u001b[32m   2096\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m form \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m seen:\n\u001b[32m   2097\u001b[39m                 result.append(form)\n\u001b[32m-> \u001b[39m\u001b[32m2098\u001b[39m                 seen.add(form)\n\u001b[32m   2099\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(data['combined_text']).toarray()\n",
    "y = data['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933512fd-256b-4334-8157-2413257494e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f8585-516c-40c9-abbf-b11c360c3445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Save model and TF-IDF\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "\n",
    "# Prediction function\n",
    "def predict_news(title, text, model, tfidf):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    features = tfidf.transform([combined]).toarray()\n",
    "    prediction = model.predict(features)[0]\n",
    "    return 'Real' if prediction == 1 else 'Fake'\n",
    "\n",
    "# Test prediction\n",
    "new_title = \"Breaking News: Aliens Land on Earth\"\n",
    "new_text = \"Scientists confirm extraterrestrial beings have made contact in Nevada...\"\n",
    "result = predict_news(new_title, new_text, model, tfidf)\n",
    "print(f\"The news is predicted to be: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aba6a1-7a88-47a9-9cf6-a29504e72a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('fake_or_real_news.csv')  # Replace with your dataset path\n",
    "\n",
    "# Check label distribution\n",
    "print(\"Label Distribution Before Balancing:\")\n",
    "print(data['label'].value_counts())\n",
    "\n",
    "# Preprocess title and text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Remove empty or short texts\n",
    "data = data[data['combined_text'].str.len() > 50]\n",
    "print(f\"Dataset Size After Cleaning: {len(data)}\")\n",
    "\n",
    "# Feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))  # Include bigrams\n",
    "X = tfidf.fit_transform(data['combined_text']).toarray()\n",
    "y = data['label'].values\n",
    "\n",
    "# Balance dataset using undersampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_balanced, y_balanced = rus.fit_resample(X, y)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"Label Distribution After Balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'Real']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save model and TF-IDF vectorizer\n",
    "pickle.dump(model, open('model.pkl', 'wb'))\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "\n",
    "# Test with a sample from the dataset\n",
    "sample_idx = 0\n",
    "sample_title = data['title'].iloc[sample_idx]\n",
    "sample_text = data['text'].iloc[sample_idx]\n",
    "sample_label = data['label'].iloc[sample_idx]\n",
    "\n",
    "def predict_news(title, text, model, tfidf):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    features = tfidf.transform([combined]).toarray()\n",
    "    prediction = model.predict(features)[0]\n",
    "    return 'Real' if prediction == 1 else 'Fake'\n",
    "\n",
    "print(f\"Sample Test: True Label = {'Real' if sample_label == 1 else 'Fake'}\")\n",
    "print(f\"Predicted: {predict_news(sample_title, sample_text, model, tfidf)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d311f76f-0e78-4968-a3cf-16836b09babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('fake_or_real_news.csv')  # Replace with your dataset path\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "\n",
    "# Initialize stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Advanced preprocessing function using spaCy\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    # Convert to lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # Process with spaCy\n",
    "    doc = nlp(text)\n",
    "    # Lemmatize and remove stop words, keep named entities\n",
    "    words = [token.lemma_ for token in doc if token.text not in stop_words or token.ent_type_]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Data augmentation for real news\n",
    "def augment_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    doc = nlp(text)\n",
    "    # Simple augmentation: synonym replacement for adjectives\n",
    "    augmented = []\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'ADJ' and token.text in ['good', 'great']:\n",
    "            augmented.append('excellent' if token.text == 'good' else 'outstanding')\n",
    "        else:\n",
    "            augmented.append(token.text)\n",
    "    return ' '.join(augmented)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    data = pd.read_csv('news_dataset.csv')  # Replace with your dataset path\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Dataset file 'news_dataset.csv' not found.\")\n",
    "    print(\"Error: 'news_dataset.csv' not found. Please provide a valid dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Check dataset\n",
    "logging.info(\"Dataset Info:\\n\" + str(data.info()))\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nLabel Distribution Before Balancing:\")\n",
    "print(data['label'].value_counts())\n",
    "logging.info(\"Label Distribution Before Balancing:\\n\" + str(data['label'].value_counts()))\n",
    "\n",
    "# Preprocess title and text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Augment real news\n",
    "real_data = data[data['label'] == 1].copy()\n",
    "real_data['text'] = real_data['text'].apply(augment_text)\n",
    "real_data['cleaned_text'] = real_data['text'].apply(preprocess_text)\n",
    "real_data['cleaned_title'] = real_data['title'].apply(preprocess_text)\n",
    "real_data['combined_text'] = real_data['cleaned_title'] + ' ' + real_data['cleaned_text']\n",
    "data = pd.concat([data, real_data], ignore_index=True)\n",
    "\n",
    "# Remove empty or short texts\n",
    "data = data[data['combined_text'].str.len() > 50]\n",
    "print(f\"\\nDataset Size After Cleaning: {len(data)}\")\n",
    "logging.info(f\"Dataset Size After Cleaning: {len(data)}\")\n",
    "\n",
    "# Debug: Inspect preprocessed text\n",
    "print(\"\\nSample Preprocessed Texts:\")\n",
    "print(data[['combined_text', 'label']].head(5))\n",
    "logging.info(\"Sample Preprocessed Texts:\\n\" + str(data[['combined_text', 'label']].head(5)))\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf.fit_transform(data['combined_text'])\n",
    "y = data['label'].values\n",
    "\n",
    "# BERT feature extraction\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(outputs)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Get BERT embeddings for training data\n",
    "X_bert = get_bert_embeddings(data['combined_text'].tolist())\n",
    "\n",
    "# Combine TF-IDF and BERT features\n",
    "X_combined = hstack([X_tfidf, X_bert])\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_combined, y)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"\\nLabel Distribution After Balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "logging.info(\"Label Distribution After Balancing:\\n\" + str(pd.Series(y_balanced).value_counts()))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Fine-tune BERT model\n",
    "dataset = Dataset.from_pandas(pd.DataFrame({'text': data['combined_text'], 'label': data['label']}))\n",
    "tokenized_dataset = dataset.map(lambda x: tokenizer(x['text'], padding='max_length', truncation=True, max_length=512), batched=True)\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test['train']\n",
    "test_dataset = train_test['test']\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_bert,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save BERT model\n",
    "model_bert.save_pretrained('bert_model')\n",
    "tokenizer.save_pretrained('bert_model')\n",
    "\n",
    "# Ensemble: Combine XGBoost and BERT predictions\n",
    "def get_ensemble_predictions(X, xgb_model, bert_model, tokenizer):\n",
    "    # XGBoost predictions\n",
    "    xgb_probs = xgb_model.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # BERT predictions\n",
    "    bert_probs = []\n",
    "    for i in range(0, len(X), 16):\n",
    "        batch_texts = data['combined_text'].iloc[i:i+16].tolist()\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs).logits.softmax(dim=1)[:, 1].numpy()\n",
    "        bert_probs.append(outputs)\n",
    "    bert_probs = np.concatenate(bert_probs)\n",
    "    \n",
    "    # Stack predictions\n",
    "    stacked_features = np.column_stack((xgb_probs, bert_probs))\n",
    "    \n",
    "    # Meta-learner (Logistic Regression)\n",
    "    meta_learner = LogisticRegression()\n",
    "    meta_learner.fit(stacked_features, y_balanced[:len(stacked_features)])\n",
    "    \n",
    "    return meta_learner, stacked_features\n",
    "\n",
    "meta_learner, _ = get_ensemble_predictions(X_train, xgb_model, model_bert, tokenizer)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_bert = []\n",
    "for i in range(0, len(X_test), 16):\n",
    "    batch_texts = data['combined_text'].iloc[i:i+16].tolist()\n",
    "    inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model_bert(**inputs).logits.softmax(dim=1)[:, 1].numpy()\n",
    "    y_pred_bert.append(outputs > 0.5)\n",
    "y_pred_bert = np.concatenate(y_pred_bert).astype(int)\n",
    "\n",
    "stacked_test = np.column_stack((xgb_model.predict_proba(X_test)[:, 1], y_pred_bert))\n",
    "y_pred_ensemble = meta_learner.predict(stacked_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Accuracy: {accuracy:.2f}\")\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real']))\n",
    "logging.info(f\"Ensemble Accuracy: {accuracy:.2f}\")\n",
    "logging.info(\"Ensemble Classification Report:\\n\" + str(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real'])))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save models\n",
    "pickle.dump(xgb_model, open('xgb_model.pkl', 'wb'))\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "pickle.dump(meta_learner, open('meta_learner.pkl', 'wb'))\n",
    "\n",
    "# Prediction function for testing\n",
    "def predict_news(title, text, xgb_model, bert_model, tokenizer, tfidf, meta_learner):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    if not combined.strip():\n",
    "        return \"Empty input\", \"\"\n",
    "    \n",
    "    # TF-IDF features\n",
    "    tfidf_features = tfidf.transform([combined])\n",
    "    \n",
    "    # BERT embeddings\n",
    "    inputs = tokenizer(combined, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        bert_embedding = bert_model.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = hstack([tfidf_features, bert_embedding])\n",
    "    \n",
    "    # XGBoost prediction\n",
    "    xgb_prob = xgb_model.predict_proba(combined_features)[:, 1]\n",
    "    \n",
    "    # BERT prediction\n",
    "    with torch.no_grad():\n",
    "        bert_output = bert_model(**inputs).logits.softmax(dim=1)[:, 1].numpy()\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    stacked_features = np.column_stack((xgb_prob, bert_output))\n",
    "    prediction = meta_learner.predict(stacked_features)[0]\n",
    "    confidence = meta_learner.predict_proba(stacked_features)[0].max() * 100\n",
    "    \n",
    "    return ('Real' if prediction == 1 else 'Fake', f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# Test with samples from dataset\n",
    "print(\"\\nTesting with Dataset Samples:\")\n",
    "logging.info(\"Testing with Dataset Samples:\")\n",
    "for idx in range(10):\n",
    "    sample_title = data['title'].iloc[idx]\n",
    "    sample_text = data['text'].iloc[idx]\n",
    "    sample_label = data['label'].iloc[idx]\n",
    "    pred, conf = predict_news(sample_title, sample_text, xgb_model, model_bert, tokenizer, tfidf, meta_learner)\n",
    "    print(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")\n",
    "    logging.info(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e9d780-8c2a-49e4-9ae0-31e5bf5aa0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function using NLTK\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keep numbers for context\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Data augmentation for real news\n",
    "def augment_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    # Simple augmentation: replace some adjectives\n",
    "    for i, word in enumerate(words):\n",
    "        if word in ['good', 'great']:\n",
    "            words[i] = 'excellent' if word == 'good' else 'outstanding'\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    data = pd.read_csv('news_dataset.csv')  # Replace with your dataset path\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Dataset file 'news_dataset.csv' not found.\")\n",
    "    print(\"Error: 'news_dataset.csv' not found. Please provide a valid dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Check dataset\n",
    "logging.info(\"Dataset Info:\\n\" + str(data.info()))\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nLabel Distribution Before Balancing:\")\n",
    "print(data['label'].value_counts())\n",
    "logging.info(\"Label Distribution Before Balancing:\\n\" + str(data['label'].value_counts()))\n",
    "\n",
    "# Preprocess title and text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Augment real news\n",
    "real_data = data[data['label'] == 1].copy()\n",
    "real_data['text'] = real_data['text'].apply(augment_text)\n",
    "real_data['cleaned_text'] = real_data['text'].apply(preprocess_text)\n",
    "real_data['cleaned_title'] = real_data['title'].apply(preprocess_text)\n",
    "real_data['combined_text'] = real_data['cleaned_title'] + ' ' + real_data['cleaned_text']\n",
    "data = pd.concat([data, real_data], ignore_index=True)\n",
    "\n",
    "# Remove empty or short texts\n",
    "data = data[data['combined_text'].str.len() > 50]\n",
    "print(f\"\\nDataset Size After Cleaning: {len(data)}\")\n",
    "logging.info(f\"Dataset Size After Cleaning: {len(data)}\")\n",
    "\n",
    "# Debug: Inspect preprocessed text\n",
    "print(\"\\nSample Preprocessed Texts:\")\n",
    "print(data[['combined_text', 'label']].head(5))\n",
    "logging.info(\"Sample Preprocessed Texts:\\n\" + str(data[['combined_text', 'label']].head(5)))\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf.fit_transform(data['combined_text'])\n",
    "y = data['label'].values\n",
    "\n",
    "# BERT feature extraction\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(outputs)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert = get_bert_embeddings(data['combined_text'].tolist())\n",
    "\n",
    "# Combine TF-IDF and BERT features\n",
    "X_combined = hstack([X_tfidf, X_bert])\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_combined, y)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"\\nLabel Distribution After Balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "logging.info(\"Label Distribution After Balancing:\\n\" + str(pd.Series(y_balanced).value_counts()))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble: Use XGBoost predictions (BERT fine-tuning is skipped to simplify)\n",
    "meta_learner = LogisticRegression()\n",
    "xgb_probs = xgb_model.predict_proba(X_train)[:, 1]\n",
    "stacked_features = np.column_stack((xgb_probs, xgb_probs))  # Simplified stacking\n",
    "meta_learner.fit(stacked_features, y_train)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "stacked_test = np.column_stack((xgb_model.predict_proba(X_test)[:, 1], xgb_model.predict_proba(X_test)[:, 1]))\n",
    "y_pred_ensemble = meta_learner.predict(stacked_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Accuracy: {accuracy:.2f}\")\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real']))\n",
    "logging.info(f\"Ensemble Accuracy: {accuracy:.2f}\")\n",
    "logging.info(\"Ensemble Classification Report:\\n\" + str(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real'])))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save models\n",
    "pickle.dump(xgb_model, open('xgb_model.pkl', 'wb'))\n",
    "pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "pickle.dump(meta_learner, open('meta_learner.pkl', 'wb'))\n",
    "pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "pickle.dump(model_bert, open('bert_model.pkl', 'wb'))\n",
    "\n",
    "# Prediction function for testing\n",
    "def predict_news(title, text, xgb_model, bert_model, tokenizer, tfidf, meta_learner):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    if not combined.strip():\n",
    "        return \"Empty input\", \"\"\n",
    "    \n",
    "    # TF-IDF features\n",
    "    tfidf_features = tfidf.transform([combined])\n",
    "    \n",
    "    # BERT embeddings\n",
    "    inputs = tokenizer(combined, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        bert_embedding = bert_model.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = hstack([tfidf_features, bert_embedding])\n",
    "    \n",
    "    # XGBoost prediction\n",
    "    xgb_prob = xgb_model.predict_proba(combined_features)[:, 1]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    stacked_features = np.column_stack((xgb_prob, xgb_prob))\n",
    "    prediction = meta_learner.predict(stacked_features)[0]\n",
    "    confidence = meta_learner.predict_proba(stacked_features)[0].max() * 100\n",
    "    \n",
    "    return ('Real' if prediction == 1 else 'Fake', f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# Test with samples from dataset\n",
    "print(\"\\nTesting with Dataset Samples:\")\n",
    "logging.info(\"Testing with Dataset Samples:\")\n",
    "for idx in range(10):\n",
    "    sample_title = data['title'].iloc[idx]\n",
    "    sample_text = data['text'].iloc[idx]\n",
    "    sample_label = data['label'].iloc[idx]\n",
    "    pred, conf = predict_news(sample_title, sample_text, xgb_model, model_bert, tokenizer, tfidf, meta_learner)\n",
    "    print(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")\n",
    "    logging.info(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c85e56-f53d-4703-a727-d321ad2addf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keep numbers for context\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Data augmentation for real news\n",
    "def augment_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    # Simple augmentation: replace adjectives\n",
    "    for i, word in enumerate(words):\n",
    "        if word in ['good', 'great']:\n",
    "            words[i] = 'excellent' if word == 'good' else 'outstanding'\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    data = pd.read_csv('news_dataset.csv')  # Replace with your dataset path\n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Dataset file 'news_dataset.csv' not found.\")\n",
    "    print(\"Error: 'news_dataset.csv' not found. Please provide a valid dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Check dataset\n",
    "logging.info(\"Dataset Info:\\n\" + str(data.info()))\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nLabel Distribution Before Balancing:\")\n",
    "print(data['label'].value_counts())\n",
    "logging.info(\"Label Distribution Before Balancing:\\n\" + str(data['label'].value_counts()))\n",
    "\n",
    "# Preprocess title and text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Augment real news\n",
    "real_data = data[data['label'] == 1].copy()\n",
    "real_data['text'] = real_data['text'].apply(augment_text)\n",
    "real_data['cleaned_text'] = real_data['text'].apply(preprocess_text)\n",
    "real_data['cleaned_title'] = real_data['title'].apply(preprocess_text)\n",
    "real_data['combined_text'] = real_data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "data = pd.concat([data, real_data], ignore_index=True)\n",
    "\n",
    "# Remove empty or short texts\n",
    "data = data[data['combined_text'].str.len() > 50]\n",
    "print(f\"\\nDataset Size After Cleaning: {len(data)}\")\n",
    "logging.info(f\"Dataset Size After Cleaning: {len(data)}\")\n",
    "\n",
    "# Debug: Inspect preprocessed text\n",
    "print(\"\\nSample Preprocessed Texts:\")\n",
    "print(data[['combined_text', 'label']].head(5))\n",
    "logging.info(\"Sample Preprocessed Texts:\\n\" + str(data[['combined_text', 'label']].head(5)))\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf.fit_transform(data['combined_text'])\n",
    "y = data['label'].values\n",
    "\n",
    "# BERT feature extraction\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(outputs)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert = get_bert_embeddings(data['combined_text'].tolist())\n",
    "\n",
    "# Combine TF-IDF and BERT features\n",
    "X_combined = hstack([X_tfidf, X_bert])\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_combined, y)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"\\nLabel Distribution After Balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "logging.info(\"Label Distribution After Balancing:\\n\" + str(pd.Series(y_balanced).value_counts()))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble: Meta-learner\n",
    "meta_learner = LogisticRegression()\n",
    "xgb_probs = xgb_model.predict_proba(X_train)[:, 1]\n",
    "stacked_features = np.column_stack((xgb_probs, xgb_probs))  # Simplified for stability\n",
    "meta_learner.fit(stacked_features, y_train)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "stacked_test = np.column_stack((xgb_model.predict_proba(X_test)[:, 1], xgb_model.predict_proba(X_test)[:, 1]))\n",
    "y_pred_ensemble = meta_learner.predict(stacked_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Accuracy: {accuracy:.2f}\")\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real']))\n",
    "logging.info(f\"Ensemble Accuracy: {accuracy:.2f}\")\n",
    "logging.info(\"Ensemble Classification Report:\\n\" + str(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real'])))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save models\n",
    "try:\n",
    "    pickle.dump(xgb_model, open('xgb_model.pkl', 'wb'))\n",
    "    pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "    pickle.dump(meta_learner, open('meta_learner.pkl', 'wb'))\n",
    "    pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    pickle.dump(model_bert, open('bert_model.pkl', 'wb'))\n",
    "    print(\"Model files saved successfully: xgb_model.pkl, tfidf.pkl, meta_learner.pkl, tokenizer.pkl, bert_model.pkl\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving model files: {str(e)}\")\n",
    "    print(f\"Error saving model files: {str(e)}\")\n",
    "\n",
    "# Prediction function for testing\n",
    "def predict_news(title, text, xgb_model, bert_model, tokenizer, tfidf, meta_learner):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    if not combined.strip():\n",
    "        return \"Empty input\", \"\"\n",
    "    \n",
    "    # TF-IDF features\n",
    "    tfidf_features = tfidf.transform([combined])\n",
    "    \n",
    "    # BERT embeddings\n",
    "    inputs = tokenizer(combined, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        bert_embedding = bert_model.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = hstack([tfidf_features, bert_embedding])\n",
    "    \n",
    "    # XGBoost prediction\n",
    "    xgb_prob = xgb_model.predict_proba(combined_features)[:, 1]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    stacked_features = np.column_stack((xgb_prob, xgb_prob))\n",
    "    prediction = meta_learner.predict(stacked_features)[0]\n",
    "    confidence = meta_learner.predict_proba(stacked_features)[0].max() * 100\n",
    "    \n",
    "    return ('Real' if prediction == 1 else 'Fake', f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# Test with samples from dataset\n",
    "print(\"\\nTesting with Dataset Samples:\")\n",
    "logging.info(\"Testing with Dataset Samples:\")\n",
    "for idx in range(10):\n",
    "    sample_title = data['title'].iloc[idx]\n",
    "    sample_text = data['text'].iloc[idx]\n",
    "    sample_label = data['label'].iloc[idx]\n",
    "    pred, conf = predict_news(sample_title, sample_text, xgb_model, model_bert, tokenizer, tfidf, meta_learner)\n",
    "    print(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")\n",
    "    logging.info(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddac3949-1595-4de3-8b88-0cf19638169a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\muham\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 198.1+ KB\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  6335 non-null   int64 \n",
      " 1   title       6335 non-null   object\n",
      " 2   text        6335 non-null   object\n",
      " 3   label       6335 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 198.1+ KB\n",
      "None\n",
      "\n",
      "Label Distribution Before Balancing:\n",
      "label\n",
      "1    3171\n",
      "0    3164\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Dataset Size After Cleaning: 9489\n",
      "\n",
      "Sample Preprocessed Texts:\n",
      "                                       combined_text  label\n",
      "0  smell hillary fear daniel greenfield shillman ...      0\n",
      "1  watch exact moment paul ryan committed politic...      0\n",
      "2  kerry go paris gesture sympathy u secretary st...      1\n",
      "3  bernie supporter twitter erupt anger dnc tried...      0\n",
      "4  battle new york primary matter primary day new...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Label Distribution After Balancing:\n",
      "0    6342\n",
      "1    6342\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\muham\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [17:57:37] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ensemble Accuracy: 0.97\n",
      "Ensemble Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.98      0.97      0.97      1266\n",
      "        Real       0.97      0.98      0.97      1271\n",
      "\n",
      "    accuracy                           0.97      2537\n",
      "   macro avg       0.97      0.97      0.97      2537\n",
      "weighted avg       0.97      0.97      0.97      2537\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model files saved successfully: xgb_model.pkl, tfidf.pkl, meta_learner.pkl, tokenizer.pkl, bert_model.pkl\n",
      "\n",
      "Testing with Dataset Samples:\n",
      "Sample 1: True Label = Fake, Predicted = Fake (Confidence: 99.69%)\n",
      "Sample 2: True Label = Fake, Predicted = Fake (Confidence: 99.86%)\n",
      "Sample 3: True Label = Real, Predicted = Real (Confidence: 99.88%)\n",
      "Sample 4: True Label = Fake, Predicted = Fake (Confidence: 99.85%)\n",
      "Sample 5: True Label = Real, Predicted = Real (Confidence: 99.87%)\n",
      "Sample 6: True Label = Fake, Predicted = Fake (Confidence: 99.81%)\n",
      "Sample 7: True Label = Fake, Predicted = Fake (Confidence: 99.86%)\n",
      "Sample 8: True Label = Real, Predicted = Real (Confidence: 99.72%)\n",
      "Sample 9: True Label = Real, Predicted = Real (Confidence: 99.88%)\n",
      "Sample 10: True Label = Real, Predicted = Real (Confidence: 99.88%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import logging\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "data = pd.read_csv('fake_or_real_news.csv')  # Replace with your dataset path\n",
    "# Setup logging\n",
    "logging.basicConfig(filename='training.log', level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Initialize NLTK tools\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Keep numbers for context\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Data augmentation for real news\n",
    "def augment_text(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    words = text.split()\n",
    "    # Simple augmentation: replace adjectives\n",
    "    for i, word in enumerate(words):\n",
    "        if word in ['good', 'great']:\n",
    "            words[i] = 'excellent' if word == 'good' else 'outstanding'\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    data = pd.read_csv('fake_or_real_news.csv')  # Replace with your dataset path \n",
    "except FileNotFoundError:\n",
    "    logging.error(\"Dataset file 'news_dataset.csv' not found.\")\n",
    "    print(\"Error: 'news_dataset.csv' not found. Please provide a valid dataset.\")\n",
    "    exit()\n",
    "\n",
    "# Convert string labels to numeric\n",
    "if data['label'].dtype == 'object':\n",
    "    data['label'] = data['label'].str.upper().map({'FAKE': 0, 'REAL': 1})\n",
    "    if data['label'].isna().any():\n",
    "        logging.error(\"Some labels could not be mapped. Check dataset for invalid labels.\")\n",
    "        print(\"Error: Some labels could not be mapped. Ensure labels are 'FAKE' or 'REAL'.\")\n",
    "        exit()\n",
    "\n",
    "# Check dataset\n",
    "logging.info(\"Dataset Info:\\n\" + str(data.info()))\n",
    "print(\"Dataset Info:\")\n",
    "print(data.info())\n",
    "print(\"\\nLabel Distribution Before Balancing:\")\n",
    "print(data['label'].value_counts())\n",
    "logging.info(\"Label Distribution Before Balancing:\\n\" + str(data['label'].value_counts()))\n",
    "\n",
    "# Preprocess title and text\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "data['cleaned_title'] = data['title'].apply(preprocess_text)\n",
    "data['combined_text'] = data['cleaned_title'] + ' ' + data['cleaned_text']\n",
    "\n",
    "# Augment real news\n",
    "real_data = data[data['label'] == 1].copy()\n",
    "real_data['text'] = real_data['text'].apply(augment_text)\n",
    "real_data['cleaned_text'] = real_data['text'].apply(preprocess_text)\n",
    "real_data['cleaned_title'] = real_data['title'].apply(preprocess_text)\n",
    "real_data['combined_text'] = real_data['cleaned_title'] + ' ' + real_data['cleaned_text']\n",
    "data = pd.concat([data, real_data], ignore_index=True)\n",
    "\n",
    "# Remove empty or short texts\n",
    "data = data[data['combined_text'].str.len() > 50]\n",
    "print(f\"\\nDataset Size After Cleaning: {len(data)}\")\n",
    "logging.info(f\"Dataset Size After Cleaning: {len(data)}\")\n",
    "\n",
    "# Debug: Inspect preprocessed text\n",
    "print(\"\\nSample Preprocessed Texts:\")\n",
    "print(data[['combined_text', 'label']].head(5))\n",
    "logging.info(\"Sample Preprocessed Texts:\\n\" + str(data[['combined_text', 'label']].head(5)))\n",
    "\n",
    "# TF-IDF feature extraction\n",
    "tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1, 3))\n",
    "X_tfidf = tfidf.fit_transform(data['combined_text'])\n",
    "y = data['label'].values\n",
    "\n",
    "# BERT feature extraction\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "def get_bert_embeddings(texts, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = model_bert.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "        embeddings.append(outputs)\n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "# Get BERT embeddings\n",
    "X_bert = get_bert_embeddings(data['combined_text'].tolist())\n",
    "\n",
    "# Combine TF-IDF and BERT features\n",
    "X_combined = hstack([X_tfidf, X_bert])\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_balanced, y_balanced = smote.fit_resample(X_combined, y)\n",
    "\n",
    "# Check label distribution after balancing\n",
    "print(\"\\nLabel Distribution After Balancing:\")\n",
    "print(pd.Series(y_balanced).value_counts())\n",
    "logging.info(\"Label Distribution After Balancing:\\n\" + str(pd.Series(y_balanced).value_counts()))\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Ensemble: Meta-learner\n",
    "meta_learner = LogisticRegression()\n",
    "xgb_probs = xgb_model.predict_proba(X_train)[:, 1]\n",
    "stacked_features = np.column_stack((xgb_probs, xgb_probs))  # Simplified for stability\n",
    "meta_learner.fit(stacked_features, y_train)\n",
    "\n",
    "# Evaluate ensemble model\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "stacked_test = np.column_stack((xgb_model.predict_proba(X_test)[:, 1], xgb_model.predict_proba(X_test)[:, 1]))\n",
    "y_pred_ensemble = meta_learner.predict(stacked_test)\n",
    "\n",
    "# Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred_ensemble)\n",
    "print(f\"\\nEnsemble Accuracy: {accuracy:.2f}\")\n",
    "print(\"Ensemble Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real']))\n",
    "logging.info(f\"Ensemble Accuracy: {accuracy:.2f}\")\n",
    "logging.info(\"Ensemble Classification Report:\\n\" + str(classification_report(y_test, y_pred_ensemble, target_names=['Fake', 'Real'])))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_ensemble)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Save models\n",
    "try:\n",
    "    pickle.dump(xgb_model, open('xgb_model.pkl', 'wb'))\n",
    "    pickle.dump(tfidf, open('tfidf.pkl', 'wb'))\n",
    "    pickle.dump(meta_learner, open('meta_learner.pkl', 'wb'))\n",
    "    pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))\n",
    "    pickle.dump(model_bert, open('bert_model.pkl', 'wb'))\n",
    "    print(\"Model files saved successfully: xgb_model.pkl, tfidf.pkl, meta_learner.pkl, tokenizer.pkl, bert_model.pkl\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error saving model files: {str(e)}\")\n",
    "    print(f\"Error saving model files: {str(e)}\")\n",
    "\n",
    "# Prediction function for testing\n",
    "def predict_news(title, text, xgb_model, bert_model, tokenizer, tfidf, meta_learner):\n",
    "    cleaned_title = preprocess_text(title)\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined = cleaned_title + ' ' + cleaned_text\n",
    "    if not combined.strip():\n",
    "        return \"Empty input\", \"\"\n",
    "    \n",
    "    # TF-IDF features\n",
    "    tfidf_features = tfidf.transform([combined])\n",
    "    \n",
    "    # BERT embeddings\n",
    "    inputs = tokenizer(combined, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        bert_embedding = bert_model.bert(**inputs).last_hidden_state[:, 0, :].numpy()\n",
    "    \n",
    "    # Combine features\n",
    "    combined_features = hstack([tfidf_features, bert_embedding])\n",
    "    \n",
    "    # XGBoost prediction\n",
    "    xgb_prob = xgb_model.predict_proba(combined_features)[:, 1]\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    stacked_features = np.column_stack((xgb_prob, xgb_prob))\n",
    "    prediction = meta_learner.predict(stacked_features)[0]\n",
    "    confidence = meta_learner.predict_proba(stacked_features)[0].max() * 100\n",
    "    \n",
    "    return ('Real' if prediction == 1 else 'Fake', f\"Confidence: {confidence:.2f}%\")\n",
    "\n",
    "# Test with samples from dataset\n",
    "print(\"\\nTesting with Dataset Samples:\")\n",
    "logging.info(\"Testing with Dataset Samples:\")\n",
    "for idx in range(10):\n",
    "    sample_title = data['title'].iloc[idx]\n",
    "    sample_text = data['text'].iloc[idx]\n",
    "    sample_label = data['label'].iloc[idx]\n",
    "    pred, conf = predict_news(sample_title, sample_text, xgb_model, model_bert, tokenizer, tfidf, meta_learner)\n",
    "    print(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")\n",
    "    logging.info(f\"Sample {idx+1}: True Label = {'Real' if sample_label == 1 else 'Fake'}, Predicted = {pred} ({conf})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c11570-67ed-4345-854d-4cc7a2518105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
